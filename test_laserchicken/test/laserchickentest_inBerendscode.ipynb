{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berend's code test for point cloud processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import gc\n",
    "import queue\n",
    "\n",
    "import glob\n",
    "import psutil\n",
    "from psutil import virtual_memory\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "\n",
    "import math\n",
    "from math import ceil\n",
    "\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle\n",
    "\n",
    "import laspy\n",
    "from laspy.file import File\n",
    "\n",
    "sys.path.insert(0, \"D:/GitHub/eEcoLiDAR/myPhD_escience_analysis/from_msc_students/\")\n",
    "\n",
    "import pcfeatures_v4_forcomparetest\n",
    "from pcfeatures_v4_forcomparetest import par_calc\n",
    "\n",
    "import lidar_funcs_v2\n",
    "from lidar_funcs_v2 import hasNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "workingdirectory=\"D:/Koma/escience/test_escience_2018Nov/exampledata/\"\n",
    "filename=\"tile_00022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory mappping tile_00022.las ...\n",
      "Determining average point density of the input file\n",
      "Estimated point density of this file is 1.72 points/m2\n",
      "Defining constants..\n",
      "Using K=5\n",
      "Retrieving points to be processed from memory\n",
      "Constructing KDTree\n",
      "Multiprocessing\n",
      "svmem(total=17056247808, available=9674051584, percent=43.3, used=7382196224, free=9674051584)\n",
      "9.674051584\n",
      "2.418512896\n",
      "4723603.58345984\n",
      "Using chunk size: 944720\n",
      "Total processors (hyper threading) available:  4 . Using:  4 threads\n",
      "Create start stop indexes for n_cpu_cores-1\n",
      "[[      0  927205]\n",
      " [ 927205 1854410]\n",
      " [1854410 2781615]\n",
      " [2781615 3708822]]\n",
      "Splitting work load across n_cpu-1\n",
      "Submitting thread 0\n",
      "Submitting thread 1\n",
      "Submitting thread 2\n",
      "Submitting thread 3\n",
      "RangeIndex(start=1854410, stop=2781615, step=1)RangeIndex(start=2781615, stop=3708822, step=1)\n",
      "\n",
      "RangeIndex(start=0, stop=927205, step=1)\n",
      "RangeIndex(start=927205, stop=1854410, step=1)\n",
      "21.591952323913574\n",
      "Done!\n",
      "Retrieving results\n",
      "0.9374327659606934\n",
      "Done!\n",
      "23.729258060455322\n",
      "Calculations are done, just finishing up here..\n",
      "Keeping track of parameters used...\n",
      "Bundeling everything together..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:214: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Program Files\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:215: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['intensity', 'return_number', 'number_of_returns'], dtype='object')\n",
      "Index(['x', 'y', 'z', 'intensity', 'return_number', 'number_of_returns',\n",
      "       'coeffvar_z', 'eig1', 'eig2', 'kurto_z', 'max_z', 'mean_z', 'median_z',\n",
      "       'min_z', 'planarity', 'skew_z', 'sphericity', 'std_z', 'var_z'],\n",
      "      dtype='object')\n",
      "removing duplicate columns\n",
      "Outputting to las file\n",
      "D:/Koma/escience/test_escience_2018Nov/exampledata/tile_00022_inBerendcode2.las\n",
      "Index(['x', 'y', 'z', 'intensity', 'return_number', 'number_of_returns',\n",
      "       'coeffvar_z', 'eig1', 'eig2', 'kurto_z', 'max_z', 'mean_z', 'median_z',\n",
      "       'min_z', 'planarity', 'skew_z', 'sphericity', 'std_z', 'var_z'],\n",
      "      dtype='object')\n",
      "29.948686122894287\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ########### Define import las file ###########\t\n",
    "\n",
    "    input_file_path = workingdirectory+filename+\".las\"\n",
    "        \n",
    "    print(\"Memory mappping\", os.path.basename(input_file_path), \"...\")\n",
    "        \n",
    "        #in_LAS = File(input_file_path[i], mode='r')\n",
    "    in_LAS = File(input_file_path,mode='r')\n",
    "        \n",
    "        #Read out the size and therefore point density by len points divided by minx,y max xy distances.\n",
    "    input_rows = 'max'\n",
    "\n",
    "    if input_rows == \"max\" :\n",
    "            #input_rows = None\n",
    "        input_rows = int(len(in_LAS))\n",
    "    else:\n",
    "        input_rows = int(input_rows)\n",
    "\n",
    "\n",
    "    ########### Explore LiDAR dataset ########### \n",
    "\t\n",
    "    print(\"Determining average point density of the input file\")\n",
    "        #Top left x coordinate\n",
    "    left_x = np.min(in_LAS.x)\n",
    "    top_y = np.min(in_LAS.y[np.where(in_LAS.x == left_x)])\n",
    "        \n",
    "    top_left_xy = np.array((left_x,top_y)) #used\n",
    "    bot_left_xy = np.array((left_x,top_y+10))# used\n",
    "    top_right_xy = np.array((left_x+10,top_y)) #used\n",
    "    bot_right_xy = np.array((left_x+10,top_y+10)) #unused\n",
    "\n",
    "    A = np.array((top_left_xy,bot_left_xy,top_left_xy,top_right_xy))\n",
    "    B = np.array((top_right_xy,bot_right_xy,bot_left_xy,bot_right_xy))\n",
    "        \n",
    "    dd = np.sqrt(np.einsum('ij,ij->i',A-B,A-B))\n",
    "        \n",
    "    surf_area = dd[0] * dd[2]\n",
    "        \n",
    "    x_range = np.where(np.logical_and(in_LAS.x > top_left_xy[0], in_LAS.x < top_right_xy[0]))\n",
    "        #y_range = np.where(np.logical_and(in_LAS.y > top_left_xy[1], in_LAS.y < bot_left_xy[1]))\n",
    "        \n",
    "    x_range_points = np.vstack(np.array([in_LAS.x[x_range],in_LAS.y[x_range]])).transpose()\n",
    "        \n",
    "        \n",
    "    num_points = len(np.where(np.logical_and(x_range_points[:,1] > top_left_xy[1], x_range_points[:,1] < bot_left_xy[1]))[0])\n",
    "        \n",
    "        \n",
    "    est_p_density = num_points/surf_area\n",
    "        \n",
    "    print(\"Estimated point density of this file is {} points/m2\".format(est_p_density))\n",
    "\t\n",
    "    fullstarttime = time.time()\n",
    "\n",
    "\t########### Define features ###########\n",
    "    \n",
    "    features = ['eig1', 'eig2','max_z', 'min_z','mean_z',\n",
    "                'median_z','var_z','skew_z','std_z',\"kurto_z\",\"coeffvar_z\"]\n",
    "    \t\t\n",
    "\t########### Build up KDTree ###########\n",
    "    \n",
    "    print(\"Defining constants..\")\n",
    "\n",
    "    x = None\n",
    "  \n",
    "    #k = 5 0.5pt/m2\n",
    "    #Callibration: K = 50 for 16 pt/m2\n",
    "    #50 / 16 = 3.125 K's per point\n",
    "        \n",
    "    k = int(3.125 * est_p_density)\n",
    "        \n",
    "    #introduce a minimum K check (We need atleast 3 to do anything sensible!)\n",
    "        \n",
    "    if k < 3:\n",
    "        k = 3\n",
    "            \n",
    "    print(\"Using K={}\".format(k))\n",
    "        \n",
    "    print(\"Retrieving points to be processed from memory\")\n",
    "      \n",
    "    points = np.vstack([in_LAS.x[0:input_rows],in_LAS.y[0:input_rows],in_LAS.z[0:input_rows]]).transpose()\n",
    "    \n",
    "        \n",
    "    points_remain_prop = pd.DataFrame(np.vstack([in_LAS.intensity[0:input_rows],in_LAS.return_num[0:input_rows],in_LAS.num_returns[0:input_rows]]).transpose(),columns = [\"intensity\",\"return_number\",\"number_of_returns\"])\n",
    "        #points_remain_prop = np.vstack([in_LAS.intensity[0:input_rows]]).transpose()\n",
    "  \n",
    "    print(\"Constructing KDTree\")\n",
    "\n",
    "    #evaluate options for quicker kdtree building\n",
    "    tree=scipy.spatial.cKDTree(points,balanced_tree=0,compact_nodes=0)\n",
    "    point_cloud_prop = pd.DataFrame([], columns=features)\n",
    "\t\n",
    "\t########### Set CPU usage ###########\n",
    "\t\n",
    "    print(\"Multiprocessing\")\n",
    " \n",
    "    ##Dynamic chunk size based upon CPU/MEMORY ratio\n",
    "    #500.000 seems to be good for 40 cores with 256 gb of ram.\n",
    "    #256 GB / 40 = 6.4 GB/Core. 500.000 chunk size for K = 50 seems to be appropriate.\n",
    "    #500.000 * 50 = 25.000.000 points per 6.4 GB/Core\n",
    "    #25.000.000 / 6.4 = 3.906.205 points / GB/core\n",
    "    #Values have been experimentally found.\n",
    "\t\n",
    "    cpu_count = mp.cpu_count()\n",
    "    mem=virtual_memory()\n",
    "    print(mem)\n",
    "    mem_tot = mem.free/1000000000\n",
    "    print(mem_tot)\n",
    "    cpu_mem_ratio = mem_tot/cpu_count\n",
    "    print(cpu_mem_ratio)\n",
    "    #K = 50\n",
    "    #Apparently classifying points copy huge datasets to the memory (duplicates) as such a new variable H is introduced which is a integer between 0 - 1 to be able to scale.\n",
    "\t\n",
    "    H = 0.5\n",
    "    tot_points_cpu = (3906205*H) * cpu_mem_ratio\n",
    "    print(tot_points_cpu)\n",
    "    chunk = int(tot_points_cpu / k)\n",
    " \n",
    "    print('Using chunk size:', chunk) \n",
    "\n",
    "    print(\"Total processors (hyper threading) available: \",mp.cpu_count(),\". Using: \",cpu_count, \"threads\")\n",
    "    \n",
    "    #Maximum amount of points to go through\n",
    "    \n",
    "    num_points = len(points)\n",
    "\n",
    "    #Number of points per CPU\n",
    "    \n",
    "    points_p_cpu = int(num_points/cpu_count)\n",
    "    index_order = np.zeros((cpu_count,2),dtype=int)\n",
    "    \n",
    "    #Create an index work order per core\n",
    "    \n",
    "    print(\"Create start stop indexes for n_cpu_cores-1\")\n",
    "    \n",
    "    for l in range(0,cpu_count):\n",
    "    \n",
    "        index_start = int(l * (points_p_cpu))\n",
    "    \n",
    "        index_order[l] = [int(index_start),int(index_start + (points_p_cpu))]\n",
    "    \n",
    "        if l == cpu_count-1:\n",
    "    \n",
    "            index_order[-1,1] = index_order[-1,1] + num_points%cpu_count              #Add remaining index to last cpu core\n",
    "\n",
    "    print(index_order) \n",
    "\n",
    "    #Split the work in cpu_cores, let the function run with those parameters.\n",
    "       \n",
    "    print(\"Splitting work load across n_cpu-1\")\n",
    "    \n",
    "    classification = False\n",
    "    tensor_filter = False\n",
    "    \n",
    "    q = queue.Queue()\n",
    "    \n",
    "\t########### Parallel computation ###########\n",
    "\t\n",
    "    mod_start = time.time()\n",
    "        \n",
    "    with ThreadPoolExecutor(max_workers=cpu_count) as executor:\n",
    "    \n",
    "        futures = []\n",
    "    \n",
    "        for j in range(0,cpu_count):\n",
    "    \n",
    "            start = index_order[j,0]\n",
    "    \n",
    "            stop = index_order[j,1]\n",
    "\n",
    "            print(\"Submitting thread\", j)\n",
    "            futures.append(executor.submit(par_calc, start, stop, points,points_remain_prop,tree,chunk,k,features,tensor_filter,classification))\n",
    "    \n",
    "           \n",
    "    print(time.time()-mod_start)\n",
    "        \n",
    "    print(\"Done!\")\n",
    "       \n",
    "    print(\"Retrieving results\")\n",
    "    mod_start = time.time()\n",
    "\n",
    "    output = []\n",
    "        \n",
    "    for future in futures:\n",
    "  \n",
    "        output = future.result()\n",
    "            #print(output)\n",
    "        point_cloud_prop = pd.concat([point_cloud_prop,output],axis=0)\n",
    "            #print(point_cloud_prop)\n",
    "        \n",
    "    print(time.time()-mod_start)\n",
    "    print(\"Done!\")\n",
    "\t\n",
    "    fullendtime = time.time()\n",
    "    fulldifftime=fullendtime - fullstarttime\n",
    "\n",
    "    print(fulldifftime)\n",
    "        \n",
    "    del output\n",
    "    \n",
    "    print(\"Calculations are done, just finishing up here..\")\n",
    "    \n",
    "    print(\"Keeping track of parameters used...\")\n",
    "    \n",
    "        \n",
    " \t########### Organizing the columns for output ###########\n",
    "\t\n",
    "    print(\"Bundeling everything together..\")\n",
    "        \n",
    "    points = pd.DataFrame(points,columns = [\"x\",\"y\",\"z\"])\n",
    "    points_remain_prop = pd.DataFrame(points_remain_prop,columns = [\"intensity\",\"return_number\",\"number_of_returns\"])\n",
    "        \n",
    "    points = points.ix[point_cloud_prop.index]\n",
    "    points_remain_prop = points_remain_prop.ix[point_cloud_prop.index]\n",
    "    print(points_remain_prop.columns)\n",
    "                \n",
    "    point_cloud = points\n",
    "    del points\n",
    "    point_cloud = pd.concat([point_cloud,points_remain_prop],axis=1)\n",
    "    del points_remain_prop\n",
    "    point_cloud = pd.concat([point_cloud,point_cloud_prop],axis=1)\n",
    "    del point_cloud_prop\n",
    "        \n",
    "    print(point_cloud.columns)\n",
    "    print(\"removing duplicate columns\")\n",
    "    point_cloud = point_cloud.loc[:,~point_cloud.columns.duplicated()]\n",
    "\t\n",
    "    #point_cloud.to_csv(input_file_path[:-4]+'_ascii.csv',sep=',',index=False,header=True)\n",
    "\n",
    "\t########### Output ###########\n",
    "\t\n",
    "    print(\"Outputting to las file\")\n",
    "    \n",
    "    out_filename = workingdirectory+filename+\"_inBerendcode2.las\"\n",
    "    \n",
    "    print(out_filename)\n",
    "    out_LAS = File(out_filename, mode = \"w\", header = in_LAS.header)      \n",
    "        \n",
    "    out_LAS.define_new_dimension(name=\"eig1\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"eig2\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    #out_LAS.define_new_dimension(name=\"eig3\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"max_z\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"min_z\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"mean_z\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"median_z\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"var_z\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"skew_z\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"std_z\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"kurto_z\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"coeffvar_z\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "        \n",
    "    print(point_cloud.columns)\n",
    "\n",
    "    out_LAS.x = point_cloud['x']\n",
    "        #point_cloud.drop('x')\n",
    "        \n",
    "    out_LAS.y = point_cloud['y']\n",
    "    out_LAS.z = point_cloud['z']\n",
    "    out_LAS.intensity = point_cloud['intensity'] \n",
    "    out_LAS.return_num = point_cloud['return_number']\n",
    "        #print(point_cloud[\"number_of_returns\"])\n",
    "    point_cloud[\"number_of_returns\"] = point_cloud[\"number_of_returns\"]\n",
    "        #print(point_cloud[\"number_of_returns\"])\n",
    "    out_LAS.num_returns = point_cloud['number_of_returns']\n",
    "        \n",
    "        #Setting attributes Maybe do this with \"try\" ?\n",
    "    setattr(out_LAS,'eig1'+\"_\"+str(k),point_cloud['eig1'])\n",
    "\n",
    "    setattr(out_LAS,'eig2'+\"_\"+str(k),point_cloud['eig2'])\n",
    "\n",
    "    #setattr(out_LAS,'eig3'+\"_\"+str(k),point_cloud['eig3'])\n",
    "\n",
    "    setattr(out_LAS,'max_z'+\"_\"+str(k),point_cloud['max_z'])\n",
    "\n",
    "    setattr(out_LAS,'min_z'+\"_\"+str(k),point_cloud['min_z'])\n",
    "\n",
    "    setattr(out_LAS,'mean_z'+\"_\"+str(k),point_cloud['mean_z'])\n",
    "\n",
    "    setattr(out_LAS,'median_z'+\"_\"+str(k),point_cloud['median_z'])\n",
    "\n",
    "    setattr(out_LAS,'var_z'+\"_\"+str(k),point_cloud['var_z'])\n",
    "\n",
    "    setattr(out_LAS,'skew_z'+\"_\"+str(k),point_cloud['skew_z'])\n",
    "\n",
    "    setattr(out_LAS,'std_z'+\"_\"+str(k),point_cloud['std_z'])\n",
    "\n",
    "    setattr(out_LAS,'kurto_z'+\"_\"+str(k),point_cloud['kurto_z'])\n",
    "\n",
    "    setattr(out_LAS,'coeffvar_z'+\"_\"+str(k),point_cloud['coeffvar_z'])\n",
    "    \n",
    "    out_LAS.close()\n",
    "\t\n",
    "    fullendtime_end = time.time()\n",
    "    fulldifftime_end=fullendtime_end - fullstarttime\n",
    "\t\n",
    "    print(fulldifftime_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
