{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berend's code test for point cloud processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import gc\n",
    "import queue\n",
    "\n",
    "import glob\n",
    "import psutil\n",
    "from psutil import virtual_memory\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "\n",
    "import math\n",
    "from math import ceil\n",
    "\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle\n",
    "\n",
    "import laspy\n",
    "from laspy.file import File\n",
    "\n",
    "sys.path.insert(0, \"D:/Koma/GitHub/myPhD_escience_analysis/from_msc_students/\")\n",
    "\n",
    "import pcfeatures_v3\n",
    "from pcfeatures_v3 import par_calc\n",
    "\n",
    "import lidar_funcs_v2\n",
    "from lidar_funcs_v2 import hasNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "workingdirectory=\"D:/Koma/GitHub/myPhD_escience_analysis/test_data/\"\n",
    "filename=\"tile_208000_598000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory mappping tile_208000_598000.las ...\n",
      "Determining average point density of the input file\n",
      "Estimated point density of this file is 5.9 points/m2\n",
      "Defining constants..\n",
      "Using K=18\n",
      "Retrieving points to be processed from memory\n",
      "Constructing KDTree\n",
      "Multiprocessing\n",
      "svmem(total=274758746112, available=254983213056, percent=7.2, used=19775533056, free=254983213056)\n",
      "254.983213056\n",
      "6.3745803264\n",
      "12450208.771942656\n",
      "Using chunk size: 691678\n",
      "Total processors (hyper threading) available:  40 . Using:  40 threads\n",
      "Create start stop indexes for n_cpu_cores-1\n",
      "[[      0   49232]\n",
      " [  49232   98464]\n",
      " [  98464  147696]\n",
      " [ 147696  196928]\n",
      " [ 196928  246160]\n",
      " [ 246160  295392]\n",
      " [ 295392  344624]\n",
      " [ 344624  393856]\n",
      " [ 393856  443088]\n",
      " [ 443088  492320]\n",
      " [ 492320  541552]\n",
      " [ 541552  590784]\n",
      " [ 590784  640016]\n",
      " [ 640016  689248]\n",
      " [ 689248  738480]\n",
      " [ 738480  787712]\n",
      " [ 787712  836944]\n",
      " [ 836944  886176]\n",
      " [ 886176  935408]\n",
      " [ 935408  984640]\n",
      " [ 984640 1033872]\n",
      " [1033872 1083104]\n",
      " [1083104 1132336]\n",
      " [1132336 1181568]\n",
      " [1181568 1230800]\n",
      " [1230800 1280032]\n",
      " [1280032 1329264]\n",
      " [1329264 1378496]\n",
      " [1378496 1427728]\n",
      " [1427728 1476960]\n",
      " [1476960 1526192]\n",
      " [1526192 1575424]\n",
      " [1575424 1624656]\n",
      " [1624656 1673888]\n",
      " [1673888 1723120]\n",
      " [1723120 1772352]\n",
      " [1772352 1821584]\n",
      " [1821584 1870816]\n",
      " [1870816 1920048]\n",
      " [1920048 1969286]]\n",
      "Splitting work load across n_cpu-1\n",
      "Submitting thread 0\n",
      "Submitting thread 1\n",
      "Submitting thread 2\n",
      "Submitting thread 3\n",
      "Submitting thread 4\n",
      "Submitting thread 5\n",
      "Submitting thread 6\n",
      "Submitting thread 7\n",
      "Submitting thread 8\n",
      "Submitting thread 9\n",
      "Submitting thread 10\n",
      "Submitting thread 11\n",
      "Submitting thread 12\n",
      "Submitting thread 13\n",
      "Submitting thread 14\n",
      "Submitting thread 15\n",
      "Submitting thread 16\n",
      "Submitting thread 17\n",
      "Submitting thread 18\n",
      "Submitting thread 19\n",
      "Submitting thread 20\n",
      "Submitting thread 21\n",
      "Submitting thread 22\n",
      "Submitting thread 23\n",
      "Submitting thread 24\n",
      "Submitting thread 25\n",
      "Submitting thread 26\n",
      "Submitting thread 27\n",
      "Submitting thread 28\n",
      "Submitting thread 29\n",
      "Submitting thread 30\n",
      "Submitting thread 31\n",
      "Submitting thread 32\n",
      "Submitting thread 33\n",
      "Submitting thread 34\n",
      "Submitting thread 35\n",
      "Submitting thread 36\n",
      "RangeIndex(start=147696, stop=196928, step=1)RangeIndex(start=492320, stop=541552, step=1)RangeIndex(start=393856, stop=443088, step=1)RangeIndex(start=640016, stop=689248, step=1)RangeIndex(start=196928, stop=246160, step=1)\n",
      "RangeIndex(start=295392, stop=344624, step=1)RangeIndex(start=689248, stop=738480, step=1)RangeIndex(start=49232, stop=98464, step=1)\n",
      "\n",
      "RangeIndex(start=836944, stop=886176, step=1)RangeIndex(start=1329264, stop=1378496, step=1)RangeIndex(start=1033872, stop=1083104, step=1)\n",
      "RangeIndex(start=984640, stop=1033872, step=1)RangeIndex(start=98464, stop=147696, step=1)\n",
      "RangeIndex(start=935408, stop=984640, step=1)RangeIndex(start=1772352, stop=1821584, step=1)Submitting threadRangeIndex(start=541552, stop=590784, step=1)RangeIndex(start=1673888, stop=1723120, step=1)\n",
      "RangeIndex(start=1132336, stop=1181568, step=1)RangeIndex(start=1624656, stop=1673888, step=1)\n",
      "RangeIndex(start=1083104, stop=1132336, step=1)RangeIndex(start=1378496, stop=1427728, step=1)\n",
      "\n",
      "RangeIndex(start=1181568, stop=1230800, step=1)RangeIndex(start=787712, stop=836944, step=1)RangeIndex(start=1526192, stop=1575424, step=1)RangeIndex(start=1230800, stop=1280032, step=1)\n",
      "\n",
      " RangeIndex(start=886176, stop=935408, step=1)\n",
      "\n",
      "RangeIndex(start=1575424, stop=1624656, step=1)\n",
      "\n",
      "\n",
      "\n",
      "RangeIndex(start=738480, stop=787712, step=1)\n",
      "RangeIndex(start=344624, stop=393856, step=1)RangeIndex(start=1280032, stop=1329264, step=1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RangeIndex(start=0, stop=49232, step=1)RangeIndex(start=443088, stop=492320, step=1)RangeIndex(start=1476960, stop=1526192, step=1)\n",
      "RangeIndex(start=1427728, stop=1476960, step=1)\n",
      "37\n",
      "\n",
      "RangeIndex(start=1723120, stop=1772352, step=1)\n",
      "RangeIndex(start=246160, stop=295392, step=1)\n",
      "\n",
      "\n",
      "\n",
      "Submitting thread \n",
      "\n",
      "RangeIndex(start=590784, stop=640016, step=1)\n",
      "\n",
      "\n",
      "RangeIndex(start=1821584, stop=1870816, step=1)\n",
      "38\n",
      "RangeIndex(start=1870816, stop=1920048, step=1)Submitting thread 39\n",
      "\n",
      "RangeIndex(start=1920048, stop=1969286, step=1)\n",
      "18.407471179962158\n",
      "Done!\n",
      "Retrieving results\n",
      "2.922107458114624\n",
      "Done!\n",
      "21.876500129699707\n",
      "Calculations are done, just finishing up here..\n",
      "Keeping track of parameters used...\n",
      "Bundeling everything together..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:213: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "C:\\Program Files\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:214: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['intensity', 'return_number', 'number_of_returns'], dtype='object')\n",
      "Index(['x', 'y', 'z', 'intensity', 'return_number', 'number_of_returns',\n",
      "       'linearity', 'planarity', 'sphericity', 'anisotropy', 'eigenentropy',\n",
      "       'omnivariance', 'curvature', 'sum_eigenvalues', 'delta_z', 'std_z',\n",
      "       'radius', 'density', 'norm_z'],\n",
      "      dtype='object')\n",
      "removing duplicate columns\n",
      "Outputting to las file\n",
      "D:/Koma/GitHub/myPhD_escience_analysis/test_data/tile_208000_598000_inBerendcode.las\n",
      "Index(['x', 'y', 'z', 'intensity', 'return_number', 'number_of_returns',\n",
      "       'linearity', 'planarity', 'sphericity', 'anisotropy', 'eigenentropy',\n",
      "       'omnivariance', 'curvature', 'sum_eigenvalues', 'delta_z', 'std_z',\n",
      "       'radius', 'density', 'norm_z'],\n",
      "      dtype='object')\n",
      "27.3567214012146\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ########### Define import las file ###########\t\n",
    "\n",
    "    input_file_path = workingdirectory+filename+\".las\"\n",
    "        \n",
    "    print(\"Memory mappping\", os.path.basename(input_file_path), \"...\")\n",
    "        \n",
    "        #in_LAS = File(input_file_path[i], mode='r')\n",
    "    in_LAS = File(input_file_path,mode='r')\n",
    "        \n",
    "        #Read out the size and therefore point density by len points divided by minx,y max xy distances.\n",
    "    input_rows = 'max'\n",
    "\n",
    "    if input_rows == \"max\" :\n",
    "            #input_rows = None\n",
    "        input_rows = int(len(in_LAS))\n",
    "    else:\n",
    "        input_rows = int(input_rows)\n",
    "\n",
    "\n",
    "    ########### Explore LiDAR dataset ########### \n",
    "\t\n",
    "    print(\"Determining average point density of the input file\")\n",
    "        #Top left x coordinate\n",
    "    left_x = np.min(in_LAS.x)\n",
    "    top_y = np.min(in_LAS.y[np.where(in_LAS.x == left_x)])\n",
    "        \n",
    "    top_left_xy = np.array((left_x,top_y)) #used\n",
    "    bot_left_xy = np.array((left_x,top_y+10))# used\n",
    "    top_right_xy = np.array((left_x+10,top_y)) #used\n",
    "    bot_right_xy = np.array((left_x+10,top_y+10)) #unused\n",
    "\n",
    "    A = np.array((top_left_xy,bot_left_xy,top_left_xy,top_right_xy))\n",
    "    B = np.array((top_right_xy,bot_right_xy,bot_left_xy,bot_right_xy))\n",
    "        \n",
    "    dd = np.sqrt(np.einsum('ij,ij->i',A-B,A-B))\n",
    "        \n",
    "    surf_area = dd[0] * dd[2]\n",
    "        \n",
    "    x_range = np.where(np.logical_and(in_LAS.x > top_left_xy[0], in_LAS.x < top_right_xy[0]))\n",
    "        #y_range = np.where(np.logical_and(in_LAS.y > top_left_xy[1], in_LAS.y < bot_left_xy[1]))\n",
    "        \n",
    "    x_range_points = np.vstack(np.array([in_LAS.x[x_range],in_LAS.y[x_range]])).transpose()\n",
    "        \n",
    "        \n",
    "    num_points = len(np.where(np.logical_and(x_range_points[:,1] > top_left_xy[1], x_range_points[:,1] < bot_left_xy[1]))[0])\n",
    "        \n",
    "        \n",
    "    est_p_density = num_points/surf_area\n",
    "        \n",
    "    print(\"Estimated point density of this file is {} points/m2\".format(est_p_density))\n",
    "\t\n",
    "    fullstarttime = time.time()\n",
    "\n",
    "\t########### Define features ###########\n",
    "    \n",
    "    features = ['linearity', 'planarity', 'sphericity','anisotropy', 'eigenentropy','omnivariance','curvature','sum_eigenvalues','delta_z','std_z',\"radius\",\"density\",\"norm_z\"]\n",
    "    \t\t\n",
    "\t########### Build up KDTree ###########\n",
    "    \n",
    "    print(\"Defining constants..\")\n",
    "\n",
    "    x = None\n",
    "  \n",
    "    #k = 5 0.5pt/m2\n",
    "    #Callibration: K = 50 for 16 pt/m2\n",
    "    #50 / 16 = 3.125 K's per point\n",
    "        \n",
    "    k = int(3.125 * est_p_density)\n",
    "        \n",
    "    #introduce a minimum K check (We need atleast 3 to do anything sensible!)\n",
    "        \n",
    "    if k < 3:\n",
    "        k = 3\n",
    "            \n",
    "    print(\"Using K={}\".format(k))\n",
    "        \n",
    "    print(\"Retrieving points to be processed from memory\")\n",
    "      \n",
    "    points = np.vstack([in_LAS.x[0:input_rows],in_LAS.y[0:input_rows],in_LAS.z[0:input_rows]]).transpose()\n",
    "    \n",
    "        \n",
    "    points_remain_prop = pd.DataFrame(np.vstack([in_LAS.intensity[0:input_rows],in_LAS.return_num[0:input_rows],in_LAS.num_returns[0:input_rows]]).transpose(),columns = [\"intensity\",\"return_number\",\"number_of_returns\"])\n",
    "        #points_remain_prop = np.vstack([in_LAS.intensity[0:input_rows]]).transpose()\n",
    "  \n",
    "    print(\"Constructing KDTree\")\n",
    "\n",
    "    #evaluate options for quicker kdtree building\n",
    "    tree=scipy.spatial.cKDTree(points,balanced_tree=0,compact_nodes=0)\n",
    "    point_cloud_prop = pd.DataFrame([], columns=features)\n",
    "\t\n",
    "\t########### Set CPU usage ###########\n",
    "\t\n",
    "    print(\"Multiprocessing\")\n",
    " \n",
    "    ##Dynamic chunk size based upon CPU/MEMORY ratio\n",
    "    #500.000 seems to be good for 40 cores with 256 gb of ram.\n",
    "    #256 GB / 40 = 6.4 GB/Core. 500.000 chunk size for K = 50 seems to be appropriate.\n",
    "    #500.000 * 50 = 25.000.000 points per 6.4 GB/Core\n",
    "    #25.000.000 / 6.4 = 3.906.205 points / GB/core\n",
    "    #Values have been experimentally found.\n",
    "\t\n",
    "    cpu_count = mp.cpu_count()\n",
    "    mem=virtual_memory()\n",
    "    print(mem)\n",
    "    mem_tot = mem.free/1000000000\n",
    "    print(mem_tot)\n",
    "    cpu_mem_ratio = mem_tot/cpu_count\n",
    "    print(cpu_mem_ratio)\n",
    "    #K = 50\n",
    "    #Apparently classifying points copy huge datasets to the memory (duplicates) as such a new variable H is introduced which is a integer between 0 - 1 to be able to scale.\n",
    "\t\n",
    "    H = 0.5\n",
    "    tot_points_cpu = (3906205*H) * cpu_mem_ratio\n",
    "    print(tot_points_cpu)\n",
    "    chunk = int(tot_points_cpu / k)\n",
    " \n",
    "    print('Using chunk size:', chunk) \n",
    "\n",
    "    print(\"Total processors (hyper threading) available: \",mp.cpu_count(),\". Using: \",cpu_count, \"threads\")\n",
    "    \n",
    "    #Maximum amount of points to go through\n",
    "    \n",
    "    num_points = len(points)\n",
    "\n",
    "    #Number of points per CPU\n",
    "    \n",
    "    points_p_cpu = int(num_points/cpu_count)\n",
    "    index_order = np.zeros((cpu_count,2),dtype=int)\n",
    "    \n",
    "    #Create an index work order per core\n",
    "    \n",
    "    print(\"Create start stop indexes for n_cpu_cores-1\")\n",
    "    \n",
    "    for l in range(0,cpu_count):\n",
    "    \n",
    "        index_start = int(l * (points_p_cpu))\n",
    "    \n",
    "        index_order[l] = [int(index_start),int(index_start + (points_p_cpu))]\n",
    "    \n",
    "        if l == cpu_count-1:\n",
    "    \n",
    "            index_order[-1,1] = index_order[-1,1] + num_points%cpu_count              #Add remaining index to last cpu core\n",
    "\n",
    "    print(index_order) \n",
    "\n",
    "    #Split the work in cpu_cores, let the function run with those parameters.\n",
    "       \n",
    "    print(\"Splitting work load across n_cpu-1\")\n",
    "    \n",
    "    classification = False\n",
    "    tensor_filter = False\n",
    "    \n",
    "    q = queue.Queue()\n",
    "    \n",
    "\t########### Parallel computation ###########\n",
    "\t\n",
    "    mod_start = time.time()\n",
    "        \n",
    "    with ThreadPoolExecutor(max_workers=cpu_count) as executor:\n",
    "    \n",
    "        futures = []\n",
    "    \n",
    "        for j in range(0,cpu_count):\n",
    "    \n",
    "            start = index_order[j,0]\n",
    "    \n",
    "            stop = index_order[j,1]\n",
    "\n",
    "            print(\"Submitting thread\", j)\n",
    "            futures.append(executor.submit(par_calc, start, stop, points,points_remain_prop,tree,chunk,k,features,tensor_filter,classification))\n",
    "    \n",
    "           \n",
    "    print(time.time()-mod_start)\n",
    "        \n",
    "    print(\"Done!\")\n",
    "       \n",
    "    print(\"Retrieving results\")\n",
    "    mod_start = time.time()\n",
    "\n",
    "    output = []\n",
    "        \n",
    "    for future in futures:\n",
    "  \n",
    "        output = future.result()\n",
    "            #print(output)\n",
    "        point_cloud_prop = pd.concat([point_cloud_prop,output],axis=0)\n",
    "            #print(point_cloud_prop)\n",
    "        \n",
    "    print(time.time()-mod_start)\n",
    "    print(\"Done!\")\n",
    "\t\n",
    "    fullendtime = time.time()\n",
    "    fulldifftime=fullendtime - fullstarttime\n",
    "\n",
    "    print(fulldifftime)\n",
    "        \n",
    "    del output\n",
    "    \n",
    "    print(\"Calculations are done, just finishing up here..\")\n",
    "    \n",
    "    print(\"Keeping track of parameters used...\")\n",
    "    \n",
    "        \n",
    " \t########### Organizing the columns for output ###########\n",
    "\t\n",
    "    print(\"Bundeling everything together..\")\n",
    "        \n",
    "    points = pd.DataFrame(points,columns = [\"x\",\"y\",\"z\"])\n",
    "    points_remain_prop = pd.DataFrame(points_remain_prop,columns = [\"intensity\",\"return_number\",\"number_of_returns\"])\n",
    "        \n",
    "    points = points.ix[point_cloud_prop.index]\n",
    "    points_remain_prop = points_remain_prop.ix[point_cloud_prop.index]\n",
    "    print(points_remain_prop.columns)\n",
    "                \n",
    "    point_cloud = points\n",
    "    del points\n",
    "    point_cloud = pd.concat([point_cloud,points_remain_prop],axis=1)\n",
    "    del points_remain_prop\n",
    "    point_cloud = pd.concat([point_cloud,point_cloud_prop],axis=1)\n",
    "    del point_cloud_prop\n",
    "        \n",
    "    print(point_cloud.columns)\n",
    "    print(\"removing duplicate columns\")\n",
    "    point_cloud = point_cloud.loc[:,~point_cloud.columns.duplicated()]\n",
    "\t\n",
    "    #point_cloud.to_csv(input_file_path[:-4]+'_ascii.csv',sep=',',index=False,header=True)\n",
    "\n",
    "\t########### Output ###########\n",
    "\t\n",
    "    print(\"Outputting to las file\")\n",
    "    \n",
    "    out_filename = workingdirectory+filename+\"_inBerendcode.las\"\n",
    "    \n",
    "    print(out_filename)\n",
    "    out_LAS = File(out_filename, mode = \"w\", header = in_LAS.header)      \n",
    "        \n",
    "    out_LAS.define_new_dimension(name=\"delta_z\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"std_z\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"radius\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"density\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"norm_z\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"linearity\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"planarity\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"sphericity\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"omnivariance\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"anisotropy\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"eigenentropy\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"sum_eigenvalues\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    out_LAS.define_new_dimension(name=\"curvature\"+\"_\"+str(k), data_type=9, description= \"Spatial feature\")\n",
    "    #out_LAS.define_new_dimension(name=\"classification\"+\"_\"+str(k),data_type=9, description=\"reference\")\n",
    "        \n",
    "    print(point_cloud.columns)\n",
    "\n",
    "    out_LAS.x = point_cloud['x']\n",
    "        #point_cloud.drop('x')\n",
    "        \n",
    "    out_LAS.y = point_cloud['y']\n",
    "    out_LAS.z = point_cloud['z']\n",
    "    out_LAS.intensity = point_cloud['intensity'] \n",
    "    out_LAS.return_num = point_cloud['return_number']\n",
    "        #print(point_cloud[\"number_of_returns\"])\n",
    "    point_cloud[\"number_of_returns\"] = point_cloud[\"number_of_returns\"]\n",
    "        #print(point_cloud[\"number_of_returns\"])\n",
    "    out_LAS.num_returns = point_cloud['number_of_returns']\n",
    "        \n",
    "        #Setting attributes Maybe do this with \"try\" ?\n",
    "    setattr(out_LAS,'delta_z'+\"_\"+str(k),point_cloud['delta_z'])\n",
    "        #point_cloud.drop('delta_z')\n",
    "    setattr(out_LAS,'std_z'+\"_\"+str(k),point_cloud['std_z'])\n",
    "        #point_cloud.drop('std_z')\n",
    "    setattr(out_LAS,'radius'+\"_\"+str(k),point_cloud['radius'])\n",
    "        #point_cloud.drop('radius')\n",
    "    setattr(out_LAS,'density'+\"_\"+str(k),point_cloud['density'])\n",
    "        #point_cloud.drop('density')\n",
    "    setattr(out_LAS,'norm_z'+\"_\"+str(k),point_cloud['norm_z'])\n",
    "        #point_cloud.drop('norm_z')\n",
    "    setattr(out_LAS,'linearity'+\"_\"+str(k),point_cloud['linearity'])\n",
    "        #point_cloud.drop('linearity')\n",
    "    setattr(out_LAS,'planarity'+\"_\"+str(k),point_cloud['planarity'])\n",
    "        #point_cloud.drop('planarity')\n",
    "    setattr(out_LAS,'sphericity'+\"_\"+str(k),point_cloud['sphericity'])\n",
    "        #point_cloud.drop('sphericity')\n",
    "    setattr(out_LAS,'omnivariance'+\"_\"+str(k),point_cloud['omnivariance'])\n",
    "        #point_cloud.drop('omnivariance')\n",
    "    if \"anisotropy\" in features:\n",
    "        setattr(out_LAS,'anisotropy'+\"_\"+str(k),point_cloud['anisotropy'])\n",
    "        #point_cloud.drop('anisotropy')\n",
    "    if \"eigenentropy\" in features:\n",
    "        setattr(out_LAS,'eigenentropy'+\"_\"+str(k),point_cloud['eigenentropy'])\n",
    "        #point_cloud.drop('eigenentropy')\n",
    "    if \"sum_eigenvalues\" in features:\n",
    "        setattr(out_LAS,'sum_eigenvalues'+\"_\"+str(k),point_cloud['sum_eigenvalues'])\n",
    "        #point_cloud.drop('sum_eigenvalues')\n",
    "    if \"curvature\" in features:\n",
    "        setattr(out_LAS,'curvature'+\"_\"+str(k),point_cloud['curvature'])\n",
    "        #point_cloud.drop('curvature')\n",
    "    #if \"classification\" in features:\n",
    "        #setattr(out_LAS,'classification'+\"_\"+str(k),point_cloud['classification'])\n",
    "        #point_cloud.drop('classification')\n",
    "        \n",
    "    out_LAS.close()\n",
    "\t\n",
    "    fullendtime_end = time.time()\n",
    "    fulldifftime_end=fullendtime_end - fullstarttime\n",
    "\t\n",
    "    print(fulldifftime_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
